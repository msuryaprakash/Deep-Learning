{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating CNN from scratch\n",
    "#Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvNet(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #defining hyperparams\n",
    "        self.learning_rate = 0.001\n",
    "        self.reg = 0.0001\n",
    "        self.epochs = 21\n",
    "        self.batch_size = 30\n",
    "        \n",
    "    #LOSS FUNCTION\n",
    "    #Softmax log loss\n",
    "    def softmax_loss(self, scores, y):\n",
    "        #This line is used just in case of one hot encoding\n",
    "        logs = -np.log(scores[range(scores.shape[0]), np.argmax(y, axis=1)])\n",
    "        data_loss = np.sum(logs)/scores.shape[0]\n",
    "        reg_loss = 0.5 * self.reg * np.sum(self.W1**2) + 0.5 * self.reg * np.sum(self.W2**2) +  0.5 * self.reg * np.sum(self.W3**2) +  0.5 * self.reg * np.sum(self.W4**2) \n",
    "        loss = data_loss + reg_loss\n",
    "        return loss\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    # DEFINING LAYERS FOR ConvNet  #\n",
    "    ################################\n",
    "    \n",
    "    def conv2d(self, x, w, b, stride=1, padding=0):\n",
    "        \n",
    "        #Getting sizes\n",
    "        N, D, H, W = x.shape\n",
    "        F, D, Hf, Wf = w.shape\n",
    "        \n",
    "        #Getting size for the output from this layer\n",
    "        Hout = int(1 + (H - Hf + 2*padding)/stride)\n",
    "        Wout = int(1 + (W - Wf + 2*padding)/stride)\n",
    "        \n",
    "        padded_x = np.pad(x, [(0, 0), (0, 0), (padding, padding), (padding, padding)], 'constant')\n",
    "        \n",
    "        out = np.zeros((N, F, Hout, Wout))\n",
    "        \n",
    "        #getting for each image\n",
    "        for i in range(N):\n",
    "            #for each kenrnel\n",
    "            for j in range(F):\n",
    "                #convolving\n",
    "                for o in range(Hout):\n",
    "                    hPart = o*stride\n",
    "                    for k in range(Wout):\n",
    "                        wPart = k*stride\n",
    "                        \n",
    "                        convPart = padded_x[i, :, hPart:hPart+Hf, wPart:wPart+Wf]\n",
    "                        out[i, j, o, k] = np.sum((convPart * w[j]))  + b[j]\n",
    "                        \n",
    "        cache = (x, w, b, stride, padding)\n",
    "        return out, cache\n",
    "    \n",
    "    def conv2d_backprop(self, deriv_prev, cache):\n",
    "        \n",
    "#         deriv_prev = deriv_prev.reshape(deriv_prev.shape[1], deriv_prev.shape[2], deriv_prev.shape[3], deriv_prev.shape[4])\n",
    "        x, w, b, stride, padding = cache\n",
    "        \n",
    "        N, D, H, W = x.shape\n",
    "        F, D, Hf, Wf = w.shape\n",
    "        \n",
    "        Hout = int(1 + (H - Hf + 2*padding)/stride)\n",
    "        Wout = int(1 + (W - Wf + 2*padding)/stride)\n",
    "        \n",
    "        dx = np.zeros_like(x)\n",
    "        dW = np.zeros_like(w)\n",
    "        db = np.zeros_like(b)\n",
    "        \n",
    "        padded_x = np.pad(x, [(0, 0), (0, 0), (padding, padding), (padding, padding)], 'constant')\n",
    "\n",
    "        padded_dx = np.pad(dx, [(0, 0), (0, 0), (padding, padding), (padding, padding)], 'constant')\n",
    "        \n",
    "        \n",
    "        for i in range(N):\n",
    "            for j in range(F):\n",
    "                \n",
    "                #We need to derive with same parts as we need forward prop\n",
    "                for o in range(Hout):\n",
    "                    hPart = o * stride\n",
    "                    for k in range(Wout):\n",
    "                        wPart = k*stride\n",
    "                        \n",
    "                        convPart = padded_x[i, :, hPart:hPart+Hf, wPart:wPart+Wf]\n",
    "                        #calculating derivatives\n",
    "                        \n",
    "                        db[j] += np.sum(deriv_prev[i, j, o, k], axis=0, keepdims=True)\n",
    "                        dW[j] += convPart * deriv_prev[i, j, o, k]\n",
    "                        padded_dx[i, :, hPart:hPart+Hf, wPart:wPart+Wf] += np.dot(w[j], deriv_prev[i, j, k, o])\n",
    "         \n",
    "        #move zeros from sides\n",
    "        dx = padded_dx[:, :, padding:padding+H, padding:padding+W]\n",
    "        return dx, dW, db\n",
    "    \n",
    "    #by default max_pool2d will slice input by half\n",
    "    def max_pool2d(self, x, fil_size=[2, 2], stride=2):\n",
    "        #Shape of the data\n",
    "        N, D, H, W = x.shape\n",
    "        \n",
    "        \n",
    "        Hout = int(1 + (H - fil_size[0])/stride)\n",
    "        Wout = int(1 + (W - fil_size[1])/stride)\n",
    "\n",
    "        out = np.zeros((N, D, Hout, Wout))\n",
    "        \n",
    "        for i in range(N):\n",
    "            for j in range(D):\n",
    "                \n",
    "                for o in range(Hout):\n",
    "                    hPart = o * stride\n",
    "                    for k in range(Wout):\n",
    "                        wPart = k * stride\n",
    "                        \n",
    "                        maxPart = x[i, j, hPart:hPart+fil_size[0], wPart:wPart+fil_size[1]]\n",
    "                        \n",
    "                        out[i, j, o, k] = np.max(maxPart)\n",
    "        \n",
    "        cache = (x, fil_size, stride)  \n",
    "        \n",
    "        return out, cache\n",
    "        \n",
    "    \n",
    "    def max_pool2d_backprop(self, deriv_prev, cache):\n",
    "        \n",
    "        x, fil_size, stride = cache\n",
    "        \n",
    "        N, D, H, W = x.shape\n",
    "        \n",
    "        \n",
    "        Hout = int(1 + (H - fil_size[0])/stride)\n",
    "        Wout = int(1 + (W - fil_size[1])/stride)\n",
    "           \n",
    "        dx = np.zeros_like(x)\n",
    "        \n",
    "        for i in range(N):\n",
    "            for j in range(D):\n",
    "                \n",
    "                for o in range(Hout):\n",
    "                    hPart = o * stride\n",
    "                    for k in range(Wout):\n",
    "                        wPart = k * stride\n",
    "                        \n",
    "                        maxPart = x[i, j, hPart:hPart+fil_size[0], wPart:wPart+fil_size[1]]\n",
    "                        \n",
    "                        maxOfPart = np.max(maxPart)\n",
    "                        \n",
    "                        dx[i, j, hPart:hPart+fil_size[0], wPart:wPart+fil_size[1]] += (maxPart == maxOfPart) * deriv_prev[i, j, o, k]\n",
    "                           \n",
    "        return dx\n",
    "    \n",
    "    def relu_layer(self, x):\n",
    "        out = np.maximum(x, 0)\n",
    "        cache = x\n",
    "        return out, cache        \n",
    "    \n",
    "    def relu_layer_backprop(self, deriv_prev, cache):\n",
    "        x = cache\n",
    "        dx = deriv_prev * (x > 0)\n",
    "        return dx\n",
    "    \n",
    "    def fully_connected(self, x, w, b, last_layer=False):\n",
    "        out = x.reshape(x.shape[0], np.prod(x.shape[1:])).dot(w)+b\n",
    "        cache = (x, w, b)\n",
    "        return out, cache\n",
    "    \n",
    "    def fully_connected_backprop(self, deriv_prev, cache):\n",
    "        x, w, b = cache\n",
    "        dw = x.reshape(x.shape[0], np.prod(x.shape[1:])).T.dot(deriv_prev)\n",
    "        db = np.sum(deriv_prev, axis=0, keepdims=True)\n",
    "        dx = deriv_prev.dot(w.T).reshape(x.shape) \n",
    "        return dx, dw, db\n",
    "    \n",
    "    ############################################\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        loss_history = []\n",
    "        \n",
    "        #defining weights and baiases\n",
    "        self.W1 = np.random.randn(32, 1, 3, 3) / np.sqrt(32)\n",
    "        self.b1 = np.zeros((32, 1)) / np.sqrt(32)\n",
    "        self.W2 = np.random.randn(64, 32, 3, 3) / np.sqrt(64)\n",
    "        self.b2 = np.zeros((64, 1)) / np.sqrt(64)\n",
    "        self.W3 = np.random.randn(7 * 7 * 64, 1024) / np.sqrt(7 * 7 * 64)\n",
    "        self.b3 = np.zeros((1, 1024)) / np.sqrt(1024)\n",
    "        self.W4 = np.random.randn(1024, 10) / np.sqrt(1024)\n",
    "        self.b4 = np.zeros((1, 10)) / np.sqrt(10)\n",
    "        \n",
    "        \n",
    "        #Training loop\n",
    "        for i in range(self.epochs):\n",
    "            #getting scores for forward prop\n",
    "            inx = np.random.choice(X_train.shape[0], self.batch_size, replace=True)\n",
    "            X_batch = X_train[inx,:,:,:]\n",
    "            y_batch = y_train[inx,:]\n",
    "            \n",
    "            cache, l1, l2, l3, scores = self.forward_prop(X_batch)\n",
    "            \n",
    "            #unppacing cache\n",
    "            cacheConv1, cachel1, cacheMax1, cacheConv2, cachel2, cacheMax2, cacheFc1, cachel3, cacheFc2 = cache\n",
    "            \n",
    "            loss = self.softmax_loss(scores, y_batch)\n",
    "            loss_history.append(loss)\n",
    "            if i % 5 == 0:\n",
    "                print(\"Epcohe: \", i, \" of \", self.epochs, \" Loss: \", loss)\n",
    "             \n",
    "            #Getting derivation from last layer\n",
    "            dscores = scores\n",
    "            dscores[range(X_batch.shape[0]), np.argmax(y_batch, axis=1)] -= 1\n",
    "            \n",
    "            #Backprop\n",
    "            ddl4, dW4, db4 = self.fully_connected_backprop(dscores, cacheFc2)\n",
    "            dl3 = self.relu_layer_backprop(ddl4, cachel3)\n",
    "            ddl3, dW3, db3 = self.fully_connected_backprop(dl3, cacheFc1)\n",
    "            dMl2 = self.max_pool2d_backprop(ddl3, cacheMax2)\n",
    "            dl2 = self.relu_layer_backprop(dMl2, cachel2)\n",
    "            ddl2, dW2, db2 = self.conv2d_backprop(dl2, cacheConv2)\n",
    "            dMl1 = self.max_pool2d_backprop(ddl2, cacheMax1)\n",
    "            dl1 = self.relu_layer_backprop(dMl1, cachel1)\n",
    "            dx, dW1, db1 = self.conv2d_backprop(dl1, cacheConv1)\n",
    "            \n",
    "            #Regularization\n",
    "            dW4 += self.reg * self.W4\n",
    "            dW3 += self.reg * self.W3\n",
    "            dW2 += self.reg * self.W2\n",
    "            dW1 += self.reg * self.W1\n",
    "            \n",
    "            #SGD\n",
    "            self.W4 += -self.learning_rate * dW4\n",
    "            self.b4 += -self.learning_rate * db4\n",
    "            self.W3 += -self.learning_rate * dW3    \n",
    "            self.b3 += -self.learning_rate * db3\n",
    "            self.W2 += -self.learning_rate * dW2\n",
    "            self.b2 += -self.learning_rate * db2\n",
    "            self.W1 += -self.learning_rate * dW1\n",
    "            self.b1 += -self.learning_rate * db1\n",
    "            \n",
    "                  \n",
    "        return loss_history\n",
    "   \n",
    "    #ADD CACHE return\n",
    "    def forward_prop(self, X):\n",
    "        \n",
    "        #First Conv layer\n",
    "        l1, cacheConv1 = self.conv2d(X, self.W1, self.b1, padding=1)        \n",
    "        l1, cachel1 = self.relu_layer(l1)      \n",
    "        l1, cacheMax1 = self.max_pool2d(l1)\n",
    "        \n",
    "        #Second Conv layer\n",
    "        l2, cacheConv2 = self.conv2d(l1, self.W2, self.b2, padding=1)\n",
    "        l2, cachel2 = self.relu_layer(l2)\n",
    "        l2, cacheMax2 = self.max_pool2d(l2)\n",
    "        \n",
    "        #First fully connected layer\n",
    "        l3, cacheFc1 = self.fully_connected(l2, self.W3, self.b3)\n",
    "        l3, cachel3 = self.relu_layer(l3)\n",
    "        \n",
    "        #Output layer\n",
    "        scores, cacheFc2 = self.fully_connected(l3, self.W4, self.b4, last_layer=True)\n",
    "        scores = self.softmax(scores)\n",
    "        \n",
    "        cache = (cacheConv1, cachel1, cacheMax1, cacheConv2, cachel2, cacheMax2, cacheFc1, cachel3, cacheFc2)\n",
    "        return cache, l1, l2, l3, scores\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, _, _, _, scores = self.forward_prop(X)\n",
    "        \n",
    "        pred = np.argmax(scores, axis=1)\n",
    "        return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_test, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        helper_int = 0\n",
    "            \n",
    "        if y_pred[i] == y_test[i]:\n",
    "            correct += 1\n",
    "            \n",
    "    return correct/len(y_pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)  ->  (55000, 1, 28, 28)\n",
      "(55000, 10)\n",
      "(10000, 784)  ->  (10000, 1, 28, 28)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Res#haping train and test set\n",
    "\n",
    "#Just for MNIST\n",
    "x_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "print(X_train.shape, \" -> \", x_train.shape)\n",
    "print(y_train.shape)\n",
    "x_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "print(X_test.shape, \" -> \", x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcohe:  0  of  21  Loss:  2.37773051597\n",
      "Epcohe:  5  of  21  Loss:  2.22106270489\n",
      "Epcohe:  10  of  21  Loss:  2.0836345977\n",
      "Epcohe:  15  of  21  Loss:  1.8782287578\n",
      "Epcohe:  20  of  21  Loss:  1.90596345138\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNet()\n",
    "loss = cnn.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a70c973b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    }
   ],
   "source": [
    "inx = np.random.choice(x_test.shape[0], 55, replace=True)\n",
    "test_batch_x = x_test[inx,:, :, :]\n",
    "test_batch_y = y_test[inx, :]\n",
    "pred = cnn.predict(test_batch_x)\n",
    "print(accuracy(pred, np.argmax(test_batch_y, axis=0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
